<!DOCTYPE html>
<html lang="en-us">

<head>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FGXNM2XG5R"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FGXNM2XG5R');
    </script>
    
    <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "Corporation",
            "name": "Air Street Capital",
            "logo": "https://github.com/nathanbenaich/airstreet/raw/master/Air%20Street%20Capital%20logo%20(blue).png",
            "description": "Air Street Capital is a venture capital firm investing in AI-first technology and life science companies",
            },
            "founder": {
                "@type": "Person",
                "name": ["Nathan Benaich"]
            },
            "url": "http://www.airstreet.com/",
            "knowsAbout": ["machine learning", "deep technology", "deep learning", "early stage startups", "artificial intelilgence", "reinforcement learning", "enterprise software", "SaaS", "software-as-a-service", "fundraising", "venture capital", "generative ai", "spinouts"],
            "legalName": "Air Street Capital Management Ltd.",
            "slogan": "Air Street Capital is a venture capital firm investing in AI-first technology and life science companies"
        }
    </script>
    <meta charset="UTF-8">
    <title>Staying the course - reflections ahead of the UK's AI Safety Summit</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <link rel="icon" href="favicon.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=PT+Sans:400,700" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <meta name="description" content="Air Street Capital is a venture capital firm investing in AI-first technology and life science companies.">
    <meta name="keywords" content="venture capital, investing, technology, VC, investment, funding, point nine capital, air street capital, artificial intelligence, machine learning, deep learning, intelligent systems, research, startups, life science, biology, chemistry, biotechnology, computer vision, natural language processing, SaaS, early-stage startup, VC funding">
    <link rel="icon" href="https://media.licdn.com/mpr/mpr/shrinknp_400_400/AAEAAQAAAAAAAAVtAAAAJDYzNTBjMDFjLWE4Y2ItNDI2NC04ZmE3LTRiZWJiZWNhOWJkNA.jpg" type="image/jpg" sizes="32x32">
    <meta name="author" content="Air Street Capital">

    <!-- for Twitter -->
    <meta name="twitter:card" content="summary_large_image" />
    <!-- changed this to summary_large_image to have a large share picture -->
    <meta name="twitter:site" content="@airstreet" />
    <meta name="twitter:creator" content="@airstreet" />
    <meta property="twitter:title" content="Staying the course - reflections ahead of the UK's AI Safety Summit" />
    <meta property="twitter:description" content="We argue that the UK‚Äôs pro-innovation approach is the best model for AI regulation currently out there." />
    <meta property="twitter:image" content="https://www.airstreet.com/blog/uk-ai-safety-summit.png" />

    <!-- for linkedin -->
    <meta property="og:url" content="https://www.airstreet.com/blog/uk-ai-safety-summit" />
    <meta property="og:title" content="Staying the course - reflections ahead of the UK's AI Safety Summit" />
    <meta property="og:description" content="We argue that the UK‚Äôs pro-innovation approach is the best model for AI regulation currently out there."
    />
    <meta property="og:image" content="https://www.airstreet.com/blog/uk-ai-safety-summit.png">
    <meta property="og:type" content="article" />

    <!-- Added to have cool social share buttons -->
    <!-- Bulma-Social -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-social@1/bin/bulma-social.min.css">
    <!-- Font Awesome 5 -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

    <!-- Google schema -->
    <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "Person",
            "name": "Nathan Benaich",
            "affiliation": {
                "@type": "Corporation",
                "name": "Air Street Capital"
            },
            "alumniOf": {
                "@type": "Corporation",
                "name": "Point Nine Capital"
            },
            "colleagues": {
                "@type": "Person",
                "name": ["Paula Pastor Castano"]
            },
            "hasOccupation": {
                "@type": "Occupation",
                "description": "Venture capital investor"
            },
            "knowsAbout": ["State of AI Report", "generative ai", "techbio", "artificial intelligence", "deep learning", "machine learning", "reinforcement learning", "supervised learning", "startups", "venture capital", "State of AI", "investing", "early stage investing", "SaaS", "software-as-a-service", "spinouts"],
            "description": "Nathan Benaich is the Founder of Air Street Capital, a venture capital firm investing in startups using artificial intelligence to build products for consumers and enterprises",
            "url": "https://www.nathanbenaich.com"
        }
    </script>

</head>

<body>

    <section class="page-header">
        <div class="hero-body">
            <div class="container has-text-centered">
                <div class='card equal-height logo-container'>
                    <div class='card-image'>
                        <nav class="level">
                            <div class="level-item has-text-centered">
                                <div class="container">
                                    <a href="/">
                                        <img class="logo" alt="Air Street logo" src='logo.png'>
                                    </a>
                                    <div class="project-tagline"><b>Word on Air Street</b>.</div>
                                </div>
                            </div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
        <div>
            <a href="/blog" class="button is-white is-outlined"> üè° Blog home</a>
            <a href="/portfolio" class="button is-white is-outlined"> ü¶Ñ Portfolio</a>
            <a href="/team" class="button is-white is-outlined"> üëã Team</a>
            <a href="https://nathanbenaich.substack.com/?utm_source=airstreet-web&utm_medium=website&utm_campaign=airstreet-blog/" class="button is-white is-outlined" target="_blank"> ‚úâÔ∏è Newsletter</a>
            <a href="https://www.stateof.ai//?utm_source=airstreet-web&utm_medium=website&utm_campaign=airstreet-blog" class="button is-white is-outlined" target="_blank"> üìñ State of AI</a>
            <a href="/shop" class="button is-white is-outlined"> üõçÔ∏è Merch</a>
    </section>

    <section class="main-content">
        <h2>
            <a id="about-me" class="anchor" href="#blog" aria-hidden="true"><span class="octicon octicon-link"></span></a>Staying the course - reflections ahead of the UK‚Äôs AI Safety Summit</h2>

        <p><i>Published by Alex Chalmers and Nathan Benaich on 17 August 2023.</i></p>

        <a class="button is-normal is-twitter" style="text-decoration: none;" href="https://twitter.com/intent/tweet?url=https://www.airstreet.com/blog/uk-ai-safety-summit/&text=The+UK's+pro-innovation+approach+to+AI+regulation+is+the+best+model+currently+out+there.+We+believe+alternative+proposals+risk+repeating+the+mistakes+of+past+regulation.+Here's+why+we+should+stay+the+course:"
            target="_blank">
            <span class="icon">
                <i class="fab fa-twitter"></i>
              </span>
        </a>

        <a class="button is-normal is-linkedin" style="text-decoration: none;" href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.airstreet.com/blog/uk-ai-safety-summit/&title=The+UK's+pro-innovation+approach+to+AI+regulation+is+the+best+model+currently+out+there.+We+believe+alternative+proposals+risk+repeating+the+mistakes+of+past+regulation.+Here's+why+we+should+stay+the+course:"
            target="_blank">
            <span class="icon">
                <i class="fab fa-linkedin"></i>
              </span>
        </a>

<br>
<br>

        <p>
            <i>Tl;dr:</i> Ahead of the AI Safety Summit, we argue that the UK‚Äôs pro-innovation approach is the best model for AI regulation currently out there. By treating AI as a general purpose technology whose potential should be maximised, rather than a force we need to be shielded from, it offers a proportionate balance between safety and risk. We believe alternative proposals risk repeating the mistakes of past regulation: unnecessary complexity that leads to a heavy compliance burden and a concentration of power in the hands of a small number of incumbents. 
        </p>

        <h3>Introduction</h3>

        <p>
            This autumn, the UK will convene governments and technology experts for a global summit on AI safety. While the exact agenda and guestlist is still to be revealed, the government clearly hopes that it will position the UK at the centre of the international debate on AI governance.
        </p>

        <p>
            The event is likely to bring renewed scrutiny to the UK‚Äôs ‚Äúpro-innovation‚Äù approach to AI regulation. Since the UK first published its <a href="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach" target="_blank">white paper</a>, the tone of the debate has shifted markedly. We‚Äôve seen letters signed by AI luminaries warning of a potential ‚Äúextinction-level‚Äù event, while other parts of the world have pursued stricter approaches to short-term regulation. China‚Äôs national AI law will resemble elements of the EU‚Äôs sweeping AI Act, while US states <a href="https://www.vox.com/future-perfect/23775650/ai-regulation-openai-gpt-anthropic-midjourney-stable" target="_blank">are developing</a> a patchwork of uneven, but often tough, regulations.
        </p>

        <p>
            With the conversation becoming more feverish, the UK government has slowly backed down from the framing, if not the substance, of its approach. The idea of the summit was born in this period. In May, events took a turn for the surreal, when the AI Minister <a href="https://twitter.com/JonathanCamrose/status/1659123928839618563?utm_source=substack&utm_medium=email" target="_blank">wrote into</a> The Economist to dispute their characterisation of the UK‚Äôs approach as ‚Äúlight-touch‚Äù - a term they‚Äôd in fact borrowed from the government.
        </p>

        <p>
            In the past, Air Street hasn‚Äôt shied away from challenging AI policy when we think it‚Äôs been wide-of-the-mark or lacking in ambition. On this occasion, however, we believe that the UK‚Äôs proposed approach is correct, particularly given current technical capabilities and considerable future uncertainty. We are concerned that the present media and political conversation, along with heavy lobbying, may trigger an undesirable change of course.
        </p>

        <p>
            We realise that there is sincere and often passionate disagreement in the AI community on a range of safety questions. We firmly believe that AI should be developed responsibly, robustly, and safely. Two years ago, the State of AI Report was pointing to the big labs‚Äô small safety teams and limited investment in alignment research. While we are not looking to relitigate the academic AI safety debate, it should come as no surprise that as a firm that invests in AI-first companies, we are sceptical of the most pessimistic narratives. This perspective shapes the argument that follows.
        </p>

        <p>
            In this essay, we‚Äôll lay out the strengths of the UK‚Äôs approach to AI regulation, the danger of the alternatives, and some initial thoughts on where the conversation needs to go next.
        </p>
             
        <h3>What the UK gets right</h3>

        <p>
            The point of regulation is not to eliminate risk by reducing the probability of adverse outcomes to zero. Regulation starts from the basis that legal goods or services have an intended use. Enlightened regulation understands that this intended use could be good for an individual or wider society, allowing the upside to be maximised while setting proportionate standards. It accepts that restrictions can come with trade-offs, and there is a balance we need to get right.
        </p>

        <p>
            We routinely allow or only lightly control a wide range of potentially dangerous items that we use in our everyday lives. We accept that they are essential for a range of human tasks, the number of bad actors is usually small, and that most people exercise personal responsibility in their use. 
        </p>

        <p>
            The UK has applied this cool-headed and proportionate approach to AI regulation, preparing regulators for potential future action without prematurely driving up compliance costs for industry. This incremental approach has also prevented the UK from overcommitting to initiatives that would‚Äôve been rendered obsolete by future technical developments, like Finland‚Äôs national AI course.
        </p>

        <p>
            While there is widespread agreement that AI is a general purpose technology, the UK is unusual in acting as if this assumption is actually true. In practice, this means accepting i) that AI risks depend largely on context, ii) that some contexts are significantly higher risk than others, and iii) the people who understand those contexts will be best-placed to respond to the risks.
        </p>

        <p>
            Instead of starting from a blank page, this approach acknowledges that sectoral regulators, many of which have existed for decades, have grappled with new technology and complex ethical issues before. The Medicines and Healthcare products Regulatory Agency (MHRA) knows how to set minimum standards for medical devices. Similarly, the Information Commissioner‚Äôs Office (ICO) has adapted to deal with individual privacy in the platform era and has investigated the use of facial recognition technology. 
        </p>

        <p>
            Both organisations have updated their standards and methodologies as the technology landscape has shifted - sometimes supported by legislation where necessary, but not preemptively. In their use of regulatory sandboxes, UK regulators have often been particularly forward-looking in allowing firms to explore how innovation runs up against existing frameworks. For example, the <a href="https://academic.oup.com/rof/advance-article/doi/10.1093/rof/rfad017/7140150#406221051" target="_blank">FCA‚Äôs sandbox</a> has been particularly successful in supporting the emergence of a new generation of fintech companies.
        </p>

        <p>
            The UK approach also accepts that sectoral regulators shouldn‚Äôt be abandoned in the face of new challenges. We see this in the planned creation of a central risk function to identify potential threats that might require action from the central government. Similarly, responding to the rapid progress in frontier models, the government has made a considerable ¬£100 million initial investment behind the Foundation Model Taskforce, which has a clear remit around safety.
        </p>

        <p>
            By treating AI as a general purpose technology and harnessing existing subject matter expertise and legislation, the UK‚Äôs approach is a sensible way of balancing safety and innovation. Instead of treating AI as something society needs to be shielded from, it operates on the basis that we shouldn‚Äôt squander future economic growth or leaps forward in our scientific knowledge. It may be that as capabilities develop further, we might need to revisit or tighten regulation. Nothing the government has announced so far prevents this from happening if need be. Its critics, however, disagree, and approach AI as an entirely novel challenge.
        </p>

        <h3>If all you have is a hammer‚Ä¶</h3>

        <p>
            Underlying many of the arguments that the UK regulation is <a href="https://theconversation.com/uk-risks-losing-out-on-hi-tech-growth-if-it-falters-on-ai-regulation-202817" target="_blank">‚Äúfalling behind‚Äù</a> is the assumption we need to ‚Äúkeep up‚Äù, in other words, that more regulation is better. While critics of the technology sector rightly highlight ethical lapses and failures, they spend significantly less time thinking through the potential adverse effects or the challenges of implementing their own proposed interventions.
        </p>

        <p>
            Regulation is assumed to be a cost-free action implemented by entirely rational actors. As the political philosopher Chris Freiman <a href="https://link.springer.com/epdf/10.1007/s11023-021-09562-x?sharing_token=JINw8vdyhOD4_ZqVjy1lXfe4RwlQNchNByi7wbcMAY5HwLYzdj-Zkj7l7CfRBZ4s1wVYV2KVfoqQTRryNN1090_oU4hQCdcjkg9OEzTGdOluqRuzCxAqkdrKkV2FYMLMq4yCVf8wa-zS9qllx1M75S2qxIjlyIFOItdGxtn0-G4%3D" target="_blank">argues</a>: <i>‚ÄúPerfect states beat imperfect markets, but that doesn‚Äôt establish the superiority of state solutions any more than finding that omnivorous non-smokers have lower rates of cancer than vegan smokers establishes the superiority of an omnivorous diet. We should compare like-to-like.‚Äù</i> The downside of some of these implementations is outlined in more detail below.
        </p>

        <p>
            The call to ‚Äòkeep up‚Äô is often mashed together with one or two other forms of argument.
        </p>

        <p>
            One is <a href="https://deliverypdf.ssrn.com/delivery.php?ID=088024100123018123112095019075027065121004001038027088066089112107020111065118066098119033023106033000111103103001115001092092106034037051088093095011008079004075033061024026017119097119084001126020093085105070103030107086010026082101081024102021024&EXT=pdf&INDEX=TRUE" target="_blank">simply</a> <a href="https://static1.squarespace.com/static/5b6df958f8370af3217d4178/t/649c539d4af4ee01789596a9/1687966621722/NYU+CBHR+Generative+AI_June+20+ONLINE+FINAL.pdf" target="_blank">rebranding</a> long-standing critiques of the tech sector as ‚ÄúAI harms‚Äù that merit AI regulation. It‚Äôs perfectly legitimate to believe that social media platforms don‚Äôt do enough to fight disinformation or that we should extend medical device regulation to cover wellness apps. These, however, are arguments for regulating app stores or social media platforms, not for introducing an entirely new regulatory framework for AI.
        </p>

        <p>
            The other approach is an ‚ÄúAI exceptionalism‚Äù that simply takes it as read that anything AI-related should be regulated in a fundamentally different way. 
        </p>

        <p>
            For example, a number of critics have suggested that not regulating more is tantamount to <a href="https://deliverypdf.ssrn.com/delivery.php?ID=560120086072068000022121073087008005063062020029025039112122120081031094076001077068060031127103104029043090066116079088070123051020006028053101126069085116119073005020005127090082126007011124027104094066086096064084097016108084075098118022017008085&EXT=pdf&INDEX=TRUE" target="_blank">deregulation</a>, which as far as we can tell, is a novel standard. 
        </p>

        <p>
            Similarly, we see the spectre of <a href="https://policyreview.info/articles/analysis/artificial-intelligence-regulation-united-kingdom-path-good-governance" target="_blank">‚Äògaps‚Äô</a> in sectoral regulation being raised. Regulation has always evolved as new challenges or issues that we couldn‚Äôt predict have arisen - there‚Äôs no reason why this wouldn‚Äôt be the case with AI. It‚Äôs also why we are not sold on the <a href="https://www.adalovelaceinstitute.org/report/regulating-ai-in-the-uk/#:~:text=The%20UK%27s%20approach%20to%20AI%20regulation,-While%20the%20EU&text=The%20UK%20approach%2C%20set%20out,functions%27%20to%20support%20this%20work" target="_blank">need for measures</a> like an ‚ÄúAI ombudsman‚Äù or additional statements of AI-specific rights. With the advent of personal computing, the UK didn‚Äôt create a Government Office for the PC or pass a precautionary General Computing Regulation. Instead, we expanded and flexed existing consumer standards, and introduced legislation (e.g. the Computer Misuse Act) in response to specific real-world challenges when they arose.
        </p>

        <p>
            The UK government‚Äôs existing framework (visualised below) even contains specific provisions to monitor for new risks. You may worry that this monitoring operation won‚Äôt be well-resourced or have a clear enough remit, but these concerns would presumably apply equally to a hypothetical specialist AI regulator or ombudsman.  
        </p>

        <div class="container-figure has-text-centered">
            <div class='card-figure equal-height figure-container'>
                <div class='card-image'>
                    <nav class="level">
                        <div class="level-item has-text-centered">
                            <figure class='image'><img class="figure" src='central-risks-function-activities.png'>
                            </figure>
                        </div>
                    </nav>
                </div>
            </div>
        </div>

        <p>Source: <a href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1176103/a-pro-innovation-approach-to-ai-regulation-amended-web-ready.pdf" target="_blank">AI White Paper</a></p>


        <h3>The danger of a changing course</h3>

        <p>
            You may wonder why we feel the need to respond to these criticisms of the UK‚Äôs approach far more stridently than the government has to date. This is because we believe that abandoning this approach so that we can be seen to ‚Äúkeep up‚Äù could have serious long-term consequences.
        </p>

        <p>
            We‚Äôve seen a number of warning signs in the real-world about what happens when regulation goes wrong - both in the UK and internationally. 
        </p>

        <h4><i><b>When you‚Äôre draining the swamp, you don‚Äôt ask the frogs for an objective assessment</b></i></h4>

        <p>
            The UK‚Äôs missing infrastructure, housing shortages, and lack of lab space can all be tied back to a broken planning system. Sam Dumitriu has <a href="https://www.samdumitriu.com/p/why-britain-struggles-to-build-infrastructure" target="_blank">documented</a> how a combination of incredibly burdensome assessment processes (running to tens of thousands of pages), along with an intricate web of ministerial policy statements that are out-of-sync with legislation, have produced an impenetrable bureaucracy. Added to this, stakeholder groups and lobbyists have endless opportunities to jam up the process through consultations and legal challenges.
        </p>

        <p>
            You could argue that the construction of roads has nothing to do with foundation models, but unfortunately, we see many of the same traits seeping into more established UK tech regulation.
        </p>

        <p>
            The powers being requested by the Digital Markets Unit that sits within the Competition and Markets Authority are a warning sign of what the future could hold. If granted, they would give a UK regulator <a href="https://laweconcenter.org/wp-content/uploads/2021/06/ConflictingMissions.pdf" target="_blank">unprecedented reach</a>. These include the abolition of the right of appeal against decisions; routine regulatory intervention in product decisions; the right for the regulator to alter remedies up to ten years after imposing them; and mandatory arbitration. The last point is particularly ripe for abuse. It could easily result in a world where lobbying leads to newer industries being made to give ground to rent-seeking incumbents (e.g. digital platforms in favour of traditional news platforms), with the consumer an after-thought.
        </p>

        <p>
            Considering the UK‚Äôs track record in large-scale, top-down regulation - it‚Äôs difficult to see such an approach playing out differently when approaching a vastly more complicated field in the midst of rapid evolution. 
        </p>

        <h4><i><b>There‚Äôs no such thing as a regulatory superpower</b></i></h4>

        <p>
            The EU‚Äôs AI Act has been examined and debated exhaustively and we do not propose to rehash these arguments in full. There are, however, a few specific points that drive home the advantages of the UK‚Äôs more flexible approach.
        </p>

        <p>
            Firstly, let‚Äôs focus on the difficulty that top-down systems face in adapting to fast-changing fields. The original text of the Act was prepared over 2019-20, before the explosion of interest in foundation models. As a result, they were entirely absent from the original draft. The EU‚Äôs protracted regulatory process meant there was time to incorporate them at the last minute. They are unlikely to be so lucky with future breakthroughs. These amendments, however, were done in a slapdash way. As a result, the EU‚Äôs consistent ‚Äòrisk-based‚Äô philosophy is now unevenly overridden by specific rules for foundation models and general purpose AI systems (the technical distinction between the two not being clear, even to experts in the field). This means many providers of foundation models will struggle with the burden of being doubly-regulated.
        </p>

        <p>
            Secondly, we have the challenges of enforcing this style of top-down legislation. In the Parliament‚Äôs AI Act proposal, member states will be required to designate one market surveillance authority, essentially forcing them to create dedicated AI regulators. This means that existing agencies, with appropriate subject matter expertise, in deeply complex areas like health, will not be responsible for interpreting or implementing the Act in their own domains. There is of course the possibility of poaching existing experts from other regulators to staff a new AI regulator, but this has its own knock-on effects; the supply of experts in medical devices, for example, is not limitless. 
        </p>

        <p>
            Considering the huge regulatory burden the EU will be taking on, these new regulatory bodies will need significant funding and expertise. With a small handful of exceptions, there is little sign that member states are rising to the challenge, with Alex Engler at the Brookings Institute <a href="https://www.brookings.edu/articles/key-enforcement-issues-of-the-ai-act-should-lead-eu-trilogue-debate/" target="_blank">arguing</a> that the obvious lack of preparedness is ‚Äúcertainly a cause for concern‚Äù.  
        </p>

        <p>
            Thirdly, there is the risk of the Act further concentrating power. For example, a number of open-source foundation model providers are likely to be subject to the same weight of compliance as Big Tech. Considering the already small number of open-source models that have emerged from non-profit initiatives, this is a recipe for entrenching the dominance of a handful of big companies. Some of these requirements are so onerous that even Big Tech may struggle to meet them. As a <a href="https://api.repository.cam.ac.uk/server/api/core/bitstreams/031302ba-f902-431a-af5d-028310999a0f/content" target="_blank">recent paper</a> from Google DeepMind‚Äôs Harry Law and S√©bastien Krier notes, <i>‚Äúsome transparency requirements in the AI Act are either technically impossible to comply with, or very difficult to implement ‚Ä¶ [and] may not address the risk of harmful models diffusing‚Äù.</i> 
        </p>

        <p>
            This is the natural endpoint when regulation is designed to address often nebulously-defined harms, without sufficient thought being given to wider market dynamics. We saw exactly the same misguided philosophy behind the EU‚Äôs General Data Protection Regulation (GDPR). <a href="https://www.cnet.com/news/privacy/what-gdpr-means-for-facebook-google-the-eu-us-and-you/" target="_blank">Welcomed</a> by Big Tech, its blunt, one-size-fits-all approach reduced competition and <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3548444" target="_blank">entrenched</a> the <a href="https://www.nber.org/papers/w30028" target="_blank">power</a> of the very companies it was targeting, while polluting the internet with cookie banners that make many websites unusable. 
        </p>

        <p>
            You may well think that these anti-competitive effects are a price worth paying, but supporters of the Act rarely acknowledge the existence of the trade-off. When a number of European companies signed an <a href="https://www.ft.com/content/9b72a5f4-a6d8-41aa-95b8-c75f0bc92465" target="_blank">open letter</a> warning about the consequences of the Act for European technological sovereignty, Drago»ô Tudorache, an MEP leading on the Parliament‚Äôs draft dismissed this as an <i>‚Äúaggressive lobby‚Äù</i> that was undermining Europe‚Äôs <i>‚Äúundeniable lead‚Äù</i> on regulation.
        </p>

        <p>
            While a number of countries have adopted data protection models inspired by GDPR, does anyone believe that this has made the EU richer or more powerful? Similarly, arguing that the AI Act has given the EU geopolitical sway over China due to overlaps with their planned national AI regulation would stretch credulity. Shaping innovation born in other parts of the world is not a meaningful substitute for scientific and technological breakthroughs of your own. As Emmanuel Macron has acknowledged, <i>‚Äúthe US has GAFA ‚Ä¶ we have GDPR‚Äù.</i> Being a ‚Äúregulatory superpower‚Äù is a poor consolation prize. 
        </p>

        <h3>Refining and reinforcing the UK‚Äôs approach</h3>

        <p>
            The above is not to argue that the UK‚Äôs approach is perfect or that there‚Äôs no room for improvement. 
        </p>

        <p>
            For example, we believe there are genuine concerns around resourcing. While we don‚Äôt support giving regulators a blank cheque, we believe that sectoral regulators confronting AI-issues for the first time will need to be able to draw on dispassionate expert advice.
        </p>

        <p>
            While technology companies, civil society organisations and other NGOs have their role to play, they also have their own agendas, and external stakeholders can‚Äôt act as a substitute for in-house expertise. Otherwise we risk recreating the ‚Äústakeholderist‚Äù nightmare we see in other UK regulation.
        </p>

        <p>
            We can already see the most established companies trying to position themselves as trusted partners to governments. The <a href="https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/" target="_blank">Frontier Model Forum</a>, unveiled in July of this year, brought together the biggest companies operating in the sector to shape best practice and engage with governments. Unfortunately for smaller businesses or those working on open source projects, the Forum appears to be a closed shop. Unless you ‚Äúdemonstrate commitment‚Äù to safety (as defined by the Forum) and have already deployed a frontier model (as defined by the Forum), you‚Äôre not invited.
        </p>

        <p>
            Avoiding this kind of regulatory capture will undoubtedly require significant resourcing, as well as an overdue reappraisal of how the civil service compensates staff from technical background. The Advanced Research and Invention Agency (ARIA) has been rightly exempted from the service‚Äôs inflexible (and often stingy) payscale. Building real AI expertise in government will require similar creativity.
        </p>

        <p>
            Beyond capacity, there are other issues where the UK may start having to draw clearer lines. We feel that the white paper is optimistic about the ability of sectoral regulators in their current form to grapple with liability. While this concern remains hypothetical for the moment, we think that it may become real sooner than the white paper‚Äôs authors have anticipated. A failure to take a consistent position across government risks storing these problems up for the future.
        </p>

        <h3>Closing thoughts</h3>

        <p>
            Imperfections and potential adaptations aside, we should use the UK‚Äôs AI safety summit as an opportunity to showcase the strength of our approach, not to panic about whether or not we are falling behind. 
        </p>

        <p>
            We can demonstrate that there‚Äôs nothing about our approach to short-term risks that stops us shaping early norms around longer-term international governance. This could include supporting the creation of an evaluation ecosystem, granting additional resources to open source projects, or normalising red-teaming.
        </p>

        <p>
            If anything, our more flexible approach makes it easier to respond to these emerging norms. We can even point to <a href="https://techcrunch.com/2023/06/12/uk-ai-safety-research-pledge/" target="_blank">the agreement</a> UK Prime Minister Rishi Sunak secured from OpenAI, Google DeepMind, and Anthropic to give the government early access to their most powerful frontier models. He achieved this without having to pass a single sentence of legislation. Maybe not being a regulatory superpower comes with its upsides after all‚Ä¶
        </p>




      <footer class="site-footer">
        <span class="site-footer-credits">
          &copy; <span id="year"></span> Air Street Capital Management Ltd., all rights reserved. 
          <p><a href="https://www.twitter.com/airstreet"><u>Twitter</u></a> | <a href="https://www.linkedin.com/company/airstreetcapital/"><u>LinkedIn</u></a></p>
      </footer>
    </section>
    <script>
    document.getElementById("year").innerHTML = new Date().getFullYear();
    </script>

<!-- snippet to make sure the footer year prints current year and doesn't need updating -->

  </body>
</html>