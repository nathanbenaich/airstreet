<!DOCTYPE html>
<html lang="en-us">

<head>
    <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "Corporation",
            "name": "Air Street Capital",
            "logo": "https://github.com/nathanbenaich/airstreet/raw/master/Air%20Street%20Capital%20logo%20(blue).png",
            "description": "Air Street Capital is a venture capital firm investing in AI-first technology and life science companies",
            "address": {
                "@type": "PostalAddress",
                "addressLocality": "London",
                "addressCountry": "United Kingdom"
            },
            "founder": {
                "@type": "Person",
                "name": ["Nathan Benaich", "Victoria Rege", "Luc Vincent", "Phil Keslin", "Gaile Gordon", "Gabriel Dulac-Arnold", "Julien Cornebise", "Jan Erik Solem"]
            },
            "url": "http://www.airstreet.com/",
            "knowsAbout": ["machine learning", "deep technology", "deep learning", "early stage startups", "artificial intelilgence", "reinforcement learning", "enterprise software", "SaaS", "software-as-a-service", "fundraising", "venture capital"],
            "legalName": "Air Street Capital LLP",
            "slogan": "Air Street Capital is a venture capital firm investing in AI-first technology and life science companies"
        }
    </script>
    <meta charset="UTF-8">
    <title>Air Street Capital blog: 6 areas of AI research to watch closely</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <link rel="icon" href="favicon.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=PT+Sans:400,700" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <meta name="description" content="Air Street Capital is a venture capital firm investing in AI-first technology and life science companies.">
    <meta name="keywords" content="venture capital, investing, technology, VC, investment, funding, point nine capital, air street capital, artificial intelligence, machine learning, deep learning, intelligent systems, research, startups, life science, biology, chemistry, biotechnology, computer vision, natural language processing, SaaS, early-stage startup, VC funding">
    <link rel="icon" href="https://media.licdn.com/mpr/mpr/shrinknp_400_400/AAEAAQAAAAAAAAVtAAAAJDYzNTBjMDFjLWE4Y2ItNDI2NC04ZmE3LTRiZWJiZWNhOWJkNA.jpg" type="image/jpg" sizes="32x32">
    <meta name="author" content="Air Street Capital">
    <!-- Twitter and LinkedIn share card to update each time -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@airstreet" />
    <meta name="twitter:creator" content="@airstreet" />
    <meta property="og:url" content="https://www.airstreet.com/blog-6-areas-of-ai-research-to-watch" />
    <meta property="og:title" content="6 areas of AI to watch" />
    <meta property="og:description" content="A popular essay on particularly noteworthy techniques that are likely to impact the future of digital products and services, why they are important and who is driving innovation for each." />
    <meta property="og:image" content="https://www.airstreet.com/logo.png/">
    <meta property="og:type" content="article" />
    <!-- Added to have cool social share buttons -->
    <!-- Bulma-Social -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-social@1/bin/bulma-social.min.css">
    <!-- Font Awesome 5 -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

    <!-- Google Analytics -->
    <script>
        window.ga = window.ga || function() {
            (ga.q = ga.q || []).push(arguments)
        };
        ga.l = +new Date;
        ga('create', 'UA-41373361-6', 'auto');
        ga('send', 'pageview');
    </script>
    <script async src='//www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->

    <!-- Google schema -->
    <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "Person",
            "name": "Nathan Benaich",
            "affiliation": {
                "@type": "Corporation",
                "name": "Air Street Capital"
            },
            "alumniOf": {
                "@type": "Corporation",
                "name": "Point Nine Capital"
            },
            "colleagues": {
                "@type": "Person",
                "name": ["Luc Vincent", "Phil Keslin", "Gabriel Dulac-Arnold", "Jan Erik Solem", "Gaile Gordon", "Victoria Rege", "Julien Cornebise", "Christoph Janz"]
            },
            "hasOccupation": {
                "@type": "Occupation",
                "description": "Venture capital investor"
            },
            "knowsAbout": ["State of AI Report 2019", "artificial intelligence", "deep learning", "machine learning", "reinforcement learning", "supervised learning", "startups", "venture capital", "State of AI", "State of AI Report", "State of AI Report 2018", "investing", "early stage investing", "SaaS", "software-as-a-service"],
            "description": "Nathan Benaich is the Founder of Air Street Capital, a venture capital firm investing in startups using artificial intelligence to build products for consumers and enterprises",
            "url": "https://www.nathanbenaich.com"
        }
    </script>

</head>

<body>

    <section class="page-header">
        <div class="hero-body">
            <div class="container has-text-centered">
                <div class='card equal-height logo-container'>
                    <div class='card-image'>
                        <nav class="level">
                            <div class="level-item has-text-centered">
                                <div class="container">
                                    <a href="/">
                                        <img class="logo" alt="Air Street logo" src='logo.png'>
                                    </a>
                                    <div class="project-tagline">Welcome to our blog, <b>Word on Air Street</b>.</div>
                                </div>
                            </div>
                    </div>
                    </nav>
                </div>
            </div>
        </div>
        </div>
        <!-- <h1 class="project-name"><b>Air Street Capital blog</b></h1> -->
        <h2 class="project-tagline">The AI-first playbook</h2>
        </h2>
        <div>
            <a href="/blog" class="button is-white is-outlined"> üè° Blog home</a>
            <a href="/portfolio" class="button is-white is-outlined"> ü¶Ñ Portfolio</a>
            <a href="/team" class="button is-white is-outlined"> üëã Team</a>
            <a href="https://newsletter.airstreet.com/?utm_source=airstreet-web&utm_medium=website&utm_campaign=airstreet-blog/" class="button is-white is-outlined" target="_blank"> ‚úâÔ∏è Newsletter</a>
            <a href="https://www.stateof.ai//?utm_source=airstreet-web&utm_medium=website&utm_campaign=airstreet-blog" class="button is-white is-outlined" target="_blank"> üìñ State of AI</a>
            <a href="/shop" class="button is-white is-outlined"> üõçÔ∏è Merch</a>
        </div>


    </section>

    <section class="main-content">
        <h2>
            <a id="about-me" class="anchor" href="#blog" aria-hidden="true"><span class="octicon octicon-link"></span></a>6 areas of AI research to watch closely</h2>

        <p><i>Published by Nathan Benaich on 16 January 2017.</i></p>


        <a class="button is-normal is-twitter" style="text-decoration: none;" href="https://twitter.com/intent/tweet?url=https://www.airstreet.com/blog/6-areas-of-ai-research-to-watch/&text=6+areas+of+AI+research+to+watch+closely+by+%40nathanbenaich+at+%40airstreet"
            target="_blank">
            <span class="icon">
                <i class="fab fa-twitter"></i>
              </span>
        </a>

        <a class="button is-normal is-linkedin" style="text-decoration: none;" href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.airstreet.com/blog/6-areas-of-ai-research-to-watch/&title=6+areas+of+AI+research+to+watch+closely+by+%40nathanbenaich+at+%40airstreet"
            target="_blank">
            <span class="icon">
                <i class="fab fa-linkedin"></i>
              </span>
        </a>


        <h3>Introduction</h3>
        <p>Distilling a generally-accepted definition of what qualifies as artificial intelligence (AI) has become a revived topic of debate in recent times. Some have rebranded AI as ‚Äúcognitive computing‚Äù or ‚Äúmachine intelligence‚Äù, while others incorrectly
            interchange AI with ‚Äúmachine learning‚Äù. This is in part because AI is not one technology. It is in fact a broad field constituted of many disciplines, ranging from robotics to machine learning. The ultimate goal of AI, most of us affirm, is
            to build machines capable of performing tasks and cognitive functions that are otherwise only within the scope of human intelligence. In order to get there, machines must be able to learn these capabilities automatically instead of having
            each of them be explicitly programmed end-to-end.</p>

        <p>It‚Äôs amazing how much progress the field of AI has achieved over the last 10 years, ranging from self-driving cars to speech recognition and synthesis. Against this backdrop, AI has become a topic of conversation in more and more companies and
            households who have come to see AI as a technology that isn‚Äôt another 20 years away, but as something that is impacting their lives today. Indeed, the popular press reports on AI almost everyday and technology giants, one by one, articulate
            their significant long-term AI strategies. While several investors and incumbents are eager to understand how to capture value in this new world, the majority are still scratching their heads to figure out what this all means. Meanwhile, governments
            are grappling with the implications of automation in society (see Obama‚Äôs <a href="https://www.nytimes.com/2017/01/12/upshot/in-obamas-farewell-a-warning-on-automations-perils.html">farewell address</a>).</p>

        <p>Given that AI will impact the entire economy, actors in these conversations represent the entire distribution of intents, levels of understanding and degrees of experience with building or using AI systems. As such, it‚Äôs crucial for a discussion
            on AI ‚Äî including the questions, conclusions and recommendations derived therefrom ‚Äî to be grounded in data and reality, not conjecture. It‚Äôs far too easy (and sometimes exciting!) to wildly extrapolate the implications of results from published
            research or tech press announcements, speculative commentary and thought experiments.</p>

        <p>From my vantage point as an investor in AI-first technology and life science companies with <a href="https://www.airstreet.com">Air Street Capital</a>, here are six areas of AI that are particularly noteworthy in their ability to impact the future
            of digital products and services. I describe what they are, why they are important, how they are being used today and include a list (by no means exhaustive) of companies and researchers working on these technologies.</p>

        <h3>1. Reinforcement learning (RL)</h3>
        <p>RL is a paradigm for learning by trial-and-error inspired by the way humans learn new tasks. In a typical RL setup, an agent is tasked with observing its current state in a digital environment and taking actions that maximise accrual of a long-term
            reward it has been set. The agent receives feedback from the environment as a result of each action such that it knows whether the action promoted or hindered its progress. An RL agent must therefore balance the exploration of its environment
            to find optimal strategies of accruing reward with exploiting the best strategy it has found to achieve the desired goal. This approach was made popular by Google DeepMind in their work on <a href="https://www.youtube.com/watch?v=Ih8EfvOzBOY">Atari games and Go</a>.
            An example of RL working in the real world is the task of optimising energy efficiency for cooling Google data centers. Here, an RL system achieved a <a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/">40% reduction</a>            in cooling costs. An important native advantage of using RL agents in environments that can be simulated (e.g. video games) is that training data can be generated in troves and at very low cost. This is in stark contrast to supervised deep
            learning tasks that often require training data that is expensive and difficult to procure from the real world.</p>

        <ul>
            <li><b>Applications:</b> Multiple agents learning in their own instance of an environment with a shared model or by interacting and learning from one another in the same environment, learning to navigate 3D environments like mazes or city streets
                for autonomous driving, inverse reinforcement learning to recapitulate observed behaviours by learning the goal of a task (e.g. learning to drive or endowing non-player video game characters with human-like behaviours).
            </li>
            <li><b>Principal Researchers:</b> Pieter Abbeel (OpenAI), David Silver, Nando de Freitas, Raia Hadsell, Marc Bellemare (Google DeepMind), Carl Rasmussen (Cambridge), Rich Sutton (Alberta), John Shawe-Taylor (UCL) and others.
            </li>
            <li><b>Companies:</b> Google DeepMind, Prowler.io, Osaro, MicroPSI, Maluuba/Microsoft, NVIDIA, Mobileye, OpenAI.
            </li>
        </ul>

        <h3>2. Generative models</h3>
        <p>In contrast to discriminative models that are used for classification or regression tasks, generative models learn a probability distribution over training examples. By sampling from this high-dimensional distribution, generative models output
            new examples that are similar to the training data. This means, for example, that a generative model trained on real images of faces can output new synthetic images of similar faces. For more details on how these models work, see Ian Goodfellow‚Äôs
            awesome NIPS 2016 tutorial write up. The architecture he introduced, generative adversarial networks (GANs), are particularly hot right now in the research world because they offer a path towards unsupervised learning. With GANs, there are
            two neural networks: a generator, which takes random noise as input and is tasked with synthesising content (e.g. an image), and a discriminator, which has learned what real images look like and is tasked with identifying whether images created
            by the generator are real or fake. Adversarial training can be thought of as a game where the generator must iteratively learn how to create images from noise such that the discriminator can no longer distinguish generated images from real
            ones. This framework is being extended to many data modalities and task.</p>

        <ul>
            <li><b>Applications:</b> Simulate possible futures of a time-series (e.g. for planning tasks in reinforcement learning); super-resolution of images; recovering 3D structure from a 2D image; generalising from small labeled datasets; tasks where
                one input can yield multiple correct outputs (e.g. predicting the next frame in a vide0; creating natural language in conversational interfaces (e.g. bots); cryptography; semi-supervised learning when not all labels are available; artistic
                style transfer; synthesising music and voice; image in-painting.
            </li>
            <li><b>Principal Researchers:</b> Twitter Cortex, Adobe, Apple, Prisma, Jukedeck*, Creative.ai, Gluru*, Mapillary*, Unbabel.
            </li>
            <li><b>Companies:</b> Ian Goodfellow (OpenAI), Yann LeCun and Soumith Chintala (Facebook AI Research), Shakir Mohamed and A√§ron van den Oord (Google DeepMind), Alyosha Efros (Berkeley) and many others.
            </li>
        </ul>

        <h3>3. Networks with memory</h3>
        <p>In order for AI systems to generalise in diverse real-world environments just as we do, they must be able to continually learn new tasks and remember how to perform all of them into the future. However, traditional neural networks are typically
            incapable of such sequential task learning without forgetting. This shortcoming is termed catastrophic forgetting. It occurs because the weights in a network that are important to solve for task A are changed when the network is subsequently
            trained to solve for task B.</p>

        <p>There are, however, several powerful architectures that can endow neural networks with varying degrees of memory. These include long-short term memory networks (a recurrent neural network variant) that are capable of processing and predicting
            time series, DeepMind‚Äôs differentiable neural computer that combines neural networks and memory systems in order to learn from and navigate complex data structures on their own, the elastic weight consolidation algorithm that slows down learning
            on certain weights depending on how important they are to previously seen tasks, and progressive neural networks that learn lateral connections between task-specific models to extract useful features from previously learned networks for a
            new task.</p>

        <ul>
            <li><b>Applications:</b> Learning agents that can generalise to new environments; robotic arm control tasks; autonomous vehicles; time series prediction (e.g. financial markets, video, IoT); natural language understanding and next word prediction.
            </li>
            <li><b>Companies:</b> Google DeepMind, NNaisense (?), SwiftKey/Microsoft Research, Facebook AI Research.
            </li>
            <li><b>Principal Researchers:</b> Alex Graves, Raia Hadsell, Koray Kavukcuoglu (Google DeepMind), J√ºrgen Schmidhuber (IDSIA), Geoffrey Hinton (Google Brain/Toronto), James Weston, Sumit Chopra, Antoine Bordes (FAIR).
            </li>
        </ul>

        <h3>4. Learning from less data and building smaller models</h3>
        <p>Deep learning models are notable for requiring enormous amounts of training data to reach state-of-the-art performance. For example, the ImageNet Large Scale Visual Recognition Challenge on which teams challenge their image recognition models,
            contains 1.2 million training images hand-labeled with 1000 object categories. Without large scale training data, deep learning models won‚Äôt converge on their optimal settings and won‚Äôt perform well on complex tasks such as speech recognition
            or machine translation. This data requirement only grows when a single neural network is used to solve a problem end-to-end; that is, taking raw audio recordings of speech as the input and outputting text transcriptions of the speech. This
            is in contrast to using multiple networks each providing intermediate representations (e.g. raw speech audio input ‚Üí phonemes ‚Üí words ‚Üí text transcript output; or raw pixels from a camera mapped directly to steering commands). If we want AI
            systems to solve tasks where training data is particularly challenging, costly, sensitive, or time-consuming to procure, it‚Äôs important to develop models that can learn optimal solutions from less examples (i.e. one or zero-shot learning).
            When training on small data sets, challenges include overfitting, difficulties in handling outliers, differences in the data distribution between training and test. An alternative approach is to improve learning of a new task by transferring
            knowledge a machine learning model acquired from a previous task using processes collectively referred to as transfer learning.</p>

        <p>A related problem is building smaller deep learning architectures with state-of-the-art performance using a similar number or significantly less parameters. Advantages would include more efficient distributed training because data needs to be
            communicated between servers, less bandwidth to export a new model from the cloud to an edge device, and improved feasibility in deploying to hardware with limited memory.</p>

        <ul>
            <li><b>Applications:</b> Training shallow networks by learning to mimic the performance of deep networks originally trained on large labeled training data; architectures with fewer parameters but equivalent performance to deep models (e.g. SqueezeNet);
                machine translation.
            </li>
            <li><b>Companies:</b> Geometric Intelligence/Uber, DeepScale.ai, Microsoft Research, Curious AI Company, Google, Bloomsbury AI.
            </li>
            <li><b>Principal Researchers:</b> Zoubin Ghahramani (Cambridge), Yoshua Bengio (Montreal), Josh Tenenbaum (MIT), Brendan Lake (NYU), Oriol Vinyals (Google DeepMind), Sebastian Riedel (UCL).
            </li>
        </ul>

        <h3>5. Hardware for training and inference</h3>
        <p>A major catalyst for progress in AI is the repurposing of graphics processing units (GPUs) for training large neural network models. Unlike central processing unit (CPUs) that compute in a sequential fashion, GPUs offer a massively parallel architecture
            that can handle multiple tasks concurrently. Given that neural networks must process enormous amounts of (often high dimensional data), training on GPUs is much faster than with CPUs. This is why GPUs have veritably become the shovels to the
            gold rush ever since the publication of AlexNet in 2012 ‚Äî the first neural network implemented on a GPU. NVIDIA continues to lead the charge into 2017, ahead of Intel, Qualcomm, AMD and more recently Google.</p>

        <p>However, GPUs were not purpose-built for training or inference; they were created to render graphics for video games. GPUs have high computational precision that is not always needed and suffer memory bandwidth and data throughput issues. This
            has opened the playing field for a new breed of startups and projects within large companies like Google to design and produce silicon specifically for high dimensional machine learning applications. Improvements promised by new chip designs
            include larger memory bandwidth, computation on graphs instead of vectors (GPUs) or scalars (CPUs), higher compute density, efficiency and performance per Watt. This is exciting because of the clear accelerating returns AI systems deliver
            to their owners and users: Faster and more efficient model training ‚Üí better user experience ‚Üí user engages with the product more ‚Üí creates larger data set ‚Üí improves model performance through optimisation. Thus, those who are able to train
            faster and deploy AI models that are computationally and energy efficient are at a significant advantage.</p>

        <ul>
            <li><b>Applications:</b> Faster training of models (especially on graphs); energy and data efficiency when making predictions; running AI systems at the edge (IoT devices); always-listening IoT devices; cloud infrastructure as a service; autonomous
                vehicles, drones and robotics.
            </li>
            <li><b>Companies:</b> Graphcore, Cerebras, Isocline Engineering, Google (TPU), NVIDIA (DGX-1), Nervana Systems (Intel), Movidius (Intel), Scortex.
            </li>
            <li><b>Principal Researchers:</b> Kunle Olukotun (Stanford).
            </li>
        </ul>

        <h3>6. Simulation environments</h3>
        <p>As discussed earlier, generating training data for AI systems is often challenging. What‚Äôs more, AI‚Äôs must generalise to many situations if they‚Äôre to be useful to us in the real world. As such, developing digital environments that simulate the
            physics and behaviour of the real world will provide us with test beds to measure and train an AI‚Äôs general intelligence. These environments present raw pixels to an AI, which then take actions in order to solve for the goals they have been
            set (or learned). Training in these simulation environments can help us understand how AI systems learn, how to improve them, but also provide us with models that can potentially transfer to real-world applications.</p>

        <ul>
            <li><b>Applications:</b> Learning to drive; manufacturing; industrial design; game development; smart cities.
            </li>
            <li><b>Companies:</b> Improbable, Unity 3D, Microsoft (Minecraft), Google DeepMind/Blizzard, OpenAI, Comma.ai, Unreal Engine, Amazon Lumberyard.
            </li>
            <li><b>Principal Researchers:</b> Andrea Vedaldi (Oxford).
            </li>
        </ul>


        <footer class="site-footer">
            <span class="site-footer-credits">
          &copy; 2021 Air Street Capital LLP, all rights reserved. 
          <p>Air Street Capital LLP (OC424177) (FRN 805476) is an Appointed Representative (AR) of Met Facilities LLP, which is authorized and regulated by the Financial Conduct Authority (FRN 587084). <a href="https://www.twitter.com/airstreet"><u>Twitter</u></a> | <a href="https://www.linkedin.com/company/airstreetcapital/"><u>LinkedIn</u></a> | <a href="https://www.airstreet.com/risks"><u>Information about risks</u></a></p>
      </footer>
    </section>
  </body>
</html>